---
title: "Stats 101C Final Kaggle Report"
author: 'Team: Coffee^2 (Tyler Wu, Chelsea Miao, Charles Liu)'
date: "14 December 2020 -- Department of Statistics, UCLA"
output: 
  pdf_document:
    extra_dependencies: ["graphicx", "wrapfig", "geometry", "epstopdf"]
linkcolor: blue   
urlcolor: blue  
citecolor: blue
header-includes: |
  \geometry{left=0.75in,right=0.75in, bottom=0.75in, top=0.75in,footskip=10pt}
---

# 1. Introduction
YouTube is a video-sharing platform that is widely known for its diverse videos uploaded by its users. The site contains various factors including the abilities to upload, view, rate, share, create playlists, comment on videos, and subscribe to other users. From these factors, we realized that views stand out as an important variable because they are a direct measure of engagement with a video and also help to determine how much revenue a content creator will make. Another factor to consider is how quickly a content creator is able to accumulate these views within a short time period. This indicates their overall success and possible future success with videos.  

In this project, we aim to predict the percentage change in views on a video between the second and sixth hour since its publishing. In order to predict this metric, we have several video features at our disposal, including thumbnail image features, video title features, channel features, and other features. We utilized the classical statistical learning methods from James et al., \textit{Introduction to Statistical Learning} to help us find the best Root Mean Square Error for this particular problem.





# 2. Methodology
## (A) Understanding the Data
Initially, we mainly investigated predictors we thought might be useful for our model, but narrowed the list down through iteration with variable selection methods. From looking at the distribution for some predictors like views_2_hours (see Figure 1) and Num_Subscribers_Base_x, we found that there were a few outliers with extremely high number of views or base subscribers.

![Data Exploration on views_2_hours](C:/Users/cliuk/Documents/UCLA Works/UCLA Fall 2020/Stats 101C/Final Project/plot1.png)

The presence of these outliers informed our decision to later use XGBoost, as it is better at mitigating the effects of potentially overfitting to individual points.



## (B) Preprocessing
### *Cleaning + Feature Engineering:*
We first checked that our data was complete, and indeed there were no missing values. We also split the `PublishedDate` feature into a numeric date and time form: `DaysSince1` and `MinsSince1`, and binned hours into 4-hour bins. In addition, for the channel metric data, we added the missing High level as its own column, since it is possible that our model may select only one of the levels as an important variable (as it did with `avg_growth_high`). Of the features created, the most useful ones that were included in our model were `avg_growth_high`, `bin_hour.0`, `DaysSince`, `MinsSince1`, and `punc_num_bar_bi`.  


### *Variable Selection*  

For finding the predictors that would best fit our model, we explored the methods of Recursive Feature Elimination (*method not covered in class*), Random Forest importance, and LASSO cross-validation.  

**Our final model included the following variables:** "avg_growth_high, avg_growth_low, avg_growth_low_mid, avg_growth_mid_high, bin_hour.0, cnn_10, cnn_12, cnn_17, cnn_19, cnn_25, cnn_68, cnn_86, cnn_88, cnn_89, count_vids_high, count_vids_low, count_vids_low_mid, DaysSince1, Duration, hog_454, mean_blue, mean_green, mean_pixel_val, mean_red, MinsSince1, num_chars, num_non_stopwords, Num_Subscribers_Base_high, Num_Subscribers_Base_low_mid, Num_Subscribers_Base_mid_high, num_uppercase_chars, Num_Views_Base_high, Num_Views_Base_mid_high, num_words, p_non_stopwords, punc_num_at, punc_num_bar, punc_num_bar_bi, punc_num_com, views_2_hours" (40 variables chosen)


## (C) Outside Methods  

1. Recursive Feature Elimination -  

    i. Recursive feature elimination is a feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached. By recursively eliminating a small number of features per loop, RFE attempts to eliminate dependencies and collinearity that may exist in the model. Features are ranked by the model’s coef_ or feature_importances_ attributes.
    ii. The advantage is that the algorithm chooses the number of predictors instead of us having to choose it. 
    iii. It is a preprocessing method.


2. XGBoost
    i. XGBoost is an implementation of the Gradient Boosted Decision Trees algorithm.
    ii. The parameters allowed us to tune against overfitting.
    iii. It was much faster to run than regular Gradient Boost. Aside from the parameters, XGBoost also differs in that its trees are built on an approximate greedy algorithm, where instead of using a Weighted Gini Index to inform the next tree, it uses a Gain measure, based on how much “similarity” is gained in the observations when moving down nodes of  the tree. This algorithm is much faster to run than the usual Gradient Boost because of its inclusion of features like parallel processing, more efficient use of cache, and sparse matrices.




## (D) Statistical Model
The final model we selected was XGBoost, or Extreme Gradient Boosting (*method not covered in class*). To tune our model, we wrote a cross validation function, (see *Appendix*), which looped through a randomly generated parameter list with `xgb.cv()` to record the optimal parameter values. The most significant parameters values were $\gamma > 0$ and $\lambda > 1$, indicating that our model was tuned to prevent overfitting (through stricter pruning). The Variable Importance plot (Figure 2) and the SHAP plot (Figure 3) were important in determining the predictors to use for our model.  

![Variable Importance Plot](C:/Users/cliuk/Documents/UCLA Works/UCLA Fall 2020/Stats 101C/Final Project/plot3.png){width=80%}  

![SHAP Plot for Top 5 Predictors](C:/Users/cliuk/Documents/UCLA Works/UCLA Fall 2020/Stats 101C/Final Project/plot2.png){width=80%}






# 3. Results
The best iteration of our model scored 1.34459 on the public leaderboard, but 1.39679 on the private leaderboard. We believe the disparity is not too large that we need to wonder if we overfit our training data, and also our validation dataset returned an RMSE in a similar range.





# 4. Conclusions
We believe the model worked well for predictive purposes as it has a low and relatively consistent RMSE score. In terms of interpretability, aside from being able to view the most important variables through SHAP plots (similar to partial dependence plots), it is a little difficult to interpret the xgboost model.




# 5. Appendix
## Loading Necessary Packages & Data:
```{r, message=FALSE, warning=FALSE}
# Packages
library(tidyverse)
library(lubridate)
library(caret)
library(coefplot)
library(gridExtra)
library(xgboost)
library(glmnet)
library(randomForest)


# Data
train <- read_csv('training.csv')
test <- read_csv('test.csv')
test_id = test$id
```


## Transforming the Data
### Creating high levels for Channel Features
```{r}
attach(train)
train$Num_Subscribers_Base_high <- as.integer(Num_Subscribers_Base_low == 0 & Num_Subscribers_Base_low_mid == 0 & Num_Subscribers_Base_mid_high == 0)
train$Num_Views_Base_high <- as.integer(Num_Views_Base_low == 0 & Num_Views_Base_low_mid == 0 & Num_Views_Base_mid_high == 0)
train$avg_growth_high <- as.integer(avg_growth_low == 0 & avg_growth_low_mid == 0 & avg_growth_mid_high == 0)
train$count_vids_high <- as.integer(count_vids_low == 0 & count_vids_low_mid == 0 & count_vids_mid_high == 0)
detach(train)
attach(test)
test$Num_Subscribers_Base_high <- as.integer(Num_Subscribers_Base_low == 0 & Num_Subscribers_Base_low_mid == 0 & Num_Subscribers_Base_mid_high == 0)
test$Num_Views_Base_high <- as.integer(Num_Views_Base_low == 0 & Num_Views_Base_low_mid == 0 & Num_Views_Base_mid_high == 0)
test$count_vids_high <- as.integer(count_vids_low == 0 & count_vids_low_mid == 0 & count_vids_mid_high == 0)
test$avg_growth_high <- as.integer(avg_growth_low == 0 & avg_growth_low_mid == 0 & avg_growth_mid_high == 0)
detach(test)
```

### Converting datetime to numeric (and time-based features)
```{r}
# split PublishedDate into date and time
train <- train %>% separate(PublishedDate,c("PublishedDate","PublishedTime"),sep = " ")
test <- test %>% separate(PublishedDate,c("PublishedDate","PublishedTime"),sep = " ")
train$PublishedDate <- strptime(train$PublishedDate, "%m/%d/%Y")
train$PublishedTime <- strptime(train$PublishedTime, "%H:%M")
test$PublishedDate <- strptime(test$PublishedDate, "%m/%d/%Y")
test$PublishedTime <- strptime(test$PublishedTime, "%H:%M")

# convert time to mins since midnight (numeric)
MinsSince1 <- as.integer(train$PublishedTime - min(train$PublishedTime))/60 # store as minutes
train <- cbind(MinsSince1,train)
MinsSince1 <- as.integer(test$PublishedTime - min(test$PublishedTime))/60 # store as minutes
test <- cbind(MinsSince1,test)
# and date to days since first day (numeric)
DaysSince1 <- as.integer(train$PublishedDate - min(train$PublishedDate))/86400 # store as days
train <- cbind(DaysSince1,train)
DaysSince1 <- as.integer(test$PublishedDate - min(test$PublishedDate))/86400 # store as days
test <- cbind(DaysSince1,test)

# creat hour bins (4 hours per bin)
train$bin_hour <- as.factor(train$MinsSince1 %/% 240)
test$bin_hour <- as.factor(test$MinsSince1 %/% 240)
# one-hot encode hour bins
dmy <- dummyVars(" ~ bin_hour", data = train)
trsf <- data.frame(predict(dmy, newdata = train))
train <- cbind(train,trsf)
dmy <- dummyVars(" ~ bin_hour", data = test)
trsf <- data.frame(predict(dmy, newdata = test))
test <- cbind(test,trsf)

# create month
train$month <-  as.factor(month(as.Date(train$PublishedDate)))
test$month <-  as.factor(month(as.Date(test$PublishedDate)))
# one-hot encode month
dmy <- dummyVars(" ~ month", data = train)
trsf <- data.frame(predict(dmy, newdata = train))
train <- cbind(train,trsf)
dmy <- dummyVars(" ~ month", data = test)
trsf <- data.frame(predict(dmy, newdata = test))
test <- cbind(test,trsf)
train$month <-  as.integer(train$month)
test$month <-  as.integer(test$month)

# day of week
train$day_of_week <- weekdays(as.Date(train$PublishedDate))
test$day_of_week <- weekdays(as.Date(test$PublishedDate))
# one hot encode DoW
dmy <- dummyVars(" ~ day_of_week", data = train)
trsf <- data.frame(predict(dmy, newdata = train))
train <- cbind(train,trsf)
dmy <- dummyVars(" ~ day_of_week", data = test)
trsf <- data.frame(predict(dmy, newdata = test))
test <- cbind(test,trsf)
```

### Creating title features
```{r}
# avg word length
train$avg_word_length <- train$num_chars/train$num_words
test$avg_word_length <- test$num_chars/test$num_words
# number of non-stopwords
train$num_non_stopwords <- train$num_words - train$num_stopwords
test$num_non_stopwords <- test$num_words - test$num_stopwords
# proportion of non-stopwords
train$p_non_stopwords <- train$num_non_stopwords/train$num_words
test$p_non_stopwords <- test$num_non_stopwords/test$num_words
# proportion of digit chars
train$p_digit_chars <- train$num_digit_chars/train$num_chars 
test$p_digit_chars <- test$num_digit_chars/test$num_chars
# whether a title has a word with multiple capital letters
train$all_cap <- as.integer(train$num_uppercase_chars > train$num_uppercase_words) # (more uppercase chars than words means one word has multiple capitals)
test$all_cap <- as.integer(test$num_uppercase_chars > test$num_uppercase_words)
```

### Converting some punctuation to binary
```{r}
# had some difficulties with special characters, so renamed them
train <- train %>% rename(`punc_num_fs` = "punc_num_/",
                          `punc_num_bs` = "punc_num_\\",
                          `punc_num_col` = "punc_num_:",
                          `punc_num_bar` = "punc_num_|",
                          `punc_num_exc` = "punc_num_!",
                          `punc_num_num` = "punc_num_#",
                          `punc_num_pls` = "punc_num_+",
                          `punc_num_par_l` = "punc_num_(",
                          `punc_num_par_r` = "punc_num_)",
                          `punc_num_at` = "punc_num_@",
                          `punc_num_eql` = "punc_num_=",
                          `punc_num_q` = "punc_num_?",
                          `punc_num_dol` = "punc_num_$",
                          `punc_num_pct` = "punc_num_%",
                          `punc_num_com` = "punc_num_,",
                          `punc_num_semi` = "punc_num_;",
                          `punc_num_dash` = "punc_num_-",
                          `punc_num_dot` = "punc_num_.",
                          `punc_num_brk_l` = "punc_num_[",
                          `punc_num_crl_l` = "punc_num_{",
                          `punc_num_brk_r` = "punc_num_]",
                          `punc_num_crl_r` = "punc_num_}",
                          `punc_num_leq` = "punc_num_<",
                          `punc_num_geq` = "punc_num_>",
                          `punc_num_crt` = "punc_num_^",
                          `punc_num_and` = "punc_num_&",
                          `punc_num_dquo` = 'punc_num_"',
                          `punc_num_squo` = "punc_num_'",
                          `punc_num_bktk` = "punc_num_`",
                          `punc_num_ast` = "punc_num_*",
                          `punc_num_tld` = "punc_num_~"
                          )
# names(train)[235]
test <- test %>% rename(`punc_num_fs` = "punc_num_/",
                        `punc_num_bs` = "punc_num_\\",
                        `punc_num_col` = "punc_num_:",
                        `punc_num_bar` = "punc_num_|",
                        `punc_num_exc` = "punc_num_!",
                        `punc_num_num` = "punc_num_#",
                        `punc_num_pls` = "punc_num_+",
                        `punc_num_par_l` = "punc_num_(",
                        `punc_num_par_r` = "punc_num_)",
                        `punc_num_at` = "punc_num_@",
                        `punc_num_eql` = "punc_num_=",
                        `punc_num_q` = "punc_num_?",
                        `punc_num_dol` = "punc_num_$",
                        `punc_num_pct` = "punc_num_%",
                        `punc_num_com` = "punc_num_,",
                        `punc_num_semi` = "punc_num_;",
                        `punc_num_dash` = "punc_num_-",
                        `punc_num_dot` = "punc_num_.",
                        `punc_num_brk_l` = "punc_num_[",
                        `punc_num_crl_l` = "punc_num_{",
                        `punc_num_brk_r` = "punc_num_]",
                        `punc_num_crl_r` = "punc_num_}",
                        `punc_num_leq` = "punc_num_<",
                        `punc_num_geq` = "punc_num_>",
                        `punc_num_crt` = "punc_num_^",
                        `punc_num_and` = "punc_num_&",
                        `punc_num_dquo` = 'punc_num_"',
                        `punc_num_squo` = "punc_num_'",
                        `punc_num_bktk` = "punc_num_`",
                        `punc_num_ast` = "punc_num_*",
                        `punc_num_tld` = "punc_num_~"
                        )
# we found these values by checking whether there was a significant difference
# in growth_2_6 for the most common punctuations (so cutoff somewhat subjective)
train$punc_num_bar_bi <- as.integer(train$punc_num_bar == 1) # eq 1 is opt
test$punc_num_bar_bi <- as.integer(test$punc_num_bar == 1) # eq 1 is opt

# the only punctuation predictors in our final model were: punc_num_at punc_num_bar_bi and punc_num_com
```

### Other features
```{r}
# convert duration to binary
train$Duration2 <- as.integer(train$Duration > 6000)
test$Duration2 <- as.integer(test$Duration > 6000)
```


## Exploring the data
### Exploring features
```{r}
# subscribers
p1 <- ggplot(train,aes(x=as.factor(Num_Subscribers_Base_low),y=growth_2_6)) + stat_summary(fun = "mean", geom = "col")
p2 <- ggplot(train,aes(x=as.factor(Num_Subscribers_Base_low_mid),y=growth_2_6)) + stat_summary(fun = "mean", geom = "col")
p3 <- ggplot(train,aes(x=as.factor(Num_Subscribers_Base_mid_high),y=growth_2_6)) + stat_summary(fun = "mean", geom = "col")
p4 <- ggplot(train,aes(x=as.factor(Num_Subscribers_Base_high),y=growth_2_6)) + stat_summary(fun = "mean", geom = "col")
grid.arrange(p1,p2,p3,p4,nrow=2)
# average growth
p1 <- ggplot(train,aes(x=as.factor(avg_growth_low),y=growth_2_6)) + stat_summary(fun = "mean", geom = "col")
p2 <- ggplot(train,aes(x=as.factor(avg_growth_low_mid),y=growth_2_6)) + stat_summary(fun = "mean", geom = "col")
p3 <- ggplot(train,aes(x=as.factor(avg_growth_mid_high),y=growth_2_6)) + stat_summary(fun = "mean", geom = "col")
p4 <- ggplot(train,aes(x=as.factor(avg_growth_high),y=growth_2_6)) + stat_summary(fun = "mean", geom = "col")
grid.arrange(p1,p2,p3,p4,nrow=2) # growth highest for high avg_growth
# number of views on channel
p1 <- ggplot(train,aes(x=as.factor(Num_Views_Base_low),y=growth_2_6)) + stat_summary(fun = "mean", geom = "col")
p2 <- ggplot(train,aes(x=as.factor(Num_Views_Base_low_mid),y=growth_2_6)) + stat_summary(fun = "mean", geom = "col")
p3 <- ggplot(train,aes(x=as.factor(Num_Views_Base_mid_high),y=growth_2_6)) + stat_summary(fun = "mean", geom = "col")
p4 <- ggplot(train,aes(x=as.factor(Num_Views_Base_high),y=growth_2_6)) + stat_summary(fun = "mean", geom = "col")
grid.arrange(p1,p2,p3,p4,nrow=2) # growth highest for Num_Views_Base_mid_high (too high is not good)
# some other significant features
ggplot(train,aes(x=num_words,y=growth_2_6)) + geom_point() + geom_smooth() # num words
ggplot(train,aes(x=cnn_17,y=growth_2_6)) + geom_point() + geom_smooth() # cnn_17
ggplot(train,aes(x=views_2_hours,y=growth_2_6)) + geom_point() + geom_smooth() # views_2_hours
```

### Exploring our created features
```{r}
# numeric date and time, and binary duration
ggplot(train,aes(x=MinsSince1,y=growth_2_6)) + geom_point() + geom_smooth()
ggplot(train,aes(x=DaysSince1,y=growth_2_6)) + geom_point() + geom_smooth()
ggplot(train,aes(x=as.factor(Duration2),y=growth_2_6)) + stat_summary(fun = "mean", geom = "col") #
# day of week
p1 <- ggplot(train) + geom_bar(aes(x=day_of_week)) # lower uploads on saturday and sunday, Mondays, Fridays highest
p2 <- ggplot(train,aes(x=day_of_week,y=growth_2_6)) + stat_summary(fun = "mean", geom = "col")# geom_bar(stat="identity")
# hour of day
p3 <- ggplot(train) + geom_bar(aes(x=bin_hour)) # upload time dist
p4 <- ggplot(train,aes(x=bin_hour,y=growth_2_6)) + stat_summary(fun = "mean", geom = "col")#+ geom_bar(stat="identity")
grid.arrange(p1,p2,p3,p4,nrow=2)
```


## Variable selection
### First remove all non-numeric features
```{r}
train <- train %>% select(-c(PublishedDate,PublishedTime,day_of_week,bin_hour))
# and move growth to last col
train <- train %>% relocate(growth_2_6, .after = last_col())
```

### RFE subselect
(**WARNING:** This code takes hours to run so we commented it out.)
```{r}
# set.seed(22)
# subsets <- c(30:40)
# control <- rfeControl(functions=rfFuncs,
#                       method="cv",
#                       number=10,
#                       verbose=T)
# x <- train[,-which(colnames(train)=="growth_2_6")]
# y <- as.matrix(train[,which(colnames(train)=="growth_2_6")])
# rfe_sub <- rfe(x,y,
#                sizes=subsets,
#                rfeControl = control)
# # final variables
# paste(rfe_sub$optVariables,collapse = " ")
```

### Random Forest importance
```{r}
# variables sorted by importance for Random Forest
rf_fit = randomForest(growth_2_6~., data = train,
                        ntree = 100,
                        mtry= floor(ncol(train)/3),
                        trControl = oob_control,
                        importance=TRUE)
incMSE_Imp <- importance(rf_fit)[,"%IncMSE"] %>% sort(decreasing = T)
paste(names(incMSE_Imp[incMSE_Imp >= 1]),collapse=" ")
plot(rf_fit)
```

### LASSO subselect
```{r}
X_train = model.matrix(growth_2_6~., train)
y_train = train$growth_2_6

lasso_fit <- cv.glmnet(X_train,y_train,data = train, family = "gaussian", alpha = 1, #lambda = 0.00208, # selected by cv.glmnet 
                     standardize = TRUE)
lasso_coeffs <- extract.coef(lasso_fit) %>% arrange(desc(abs(Value)))
paste(lasso_coeffs$Coefficient[1:93],collapse = " ")
```

### Final set of predictors chosen from above 3 methods
```{r}
subset <- unlist(strsplit("avg_growth_high avg_growth_low avg_growth_low_mid avg_growth_mid_high bin_hour.0 cnn_10 cnn_12 cnn_17 cnn_19 cnn_25 cnn_68 cnn_86 cnn_88 cnn_89 count_vids_high count_vids_low count_vids_low_mid DaysSince1 Duration hog_454 mean_blue mean_green mean_pixel_val mean_red MinsSince1 num_chars num_non_stopwords Num_Subscribers_Base_high Num_Subscribers_Base_low_mid Num_Subscribers_Base_mid_high num_uppercase_chars Num_Views_Base_high Num_Views_Base_mid_high num_words p_non_stopwords punc_num_at punc_num_bar punc_num_bar_bi punc_num_com views_2_hours"," "))
train <- train[c(subset,"growth_2_6")]
```


## Creating model
### Split train and test
```{r}
set.seed(22)
train_index <- createDataPartition(train$growth_2_6, p = 0.7, 
          list = FALSE)
train <- train[train_index,]
valid <- train[-train_index,]
dim(train)
dim(valid)
```

### Format data for XGBoost
```{r}
# xgboost requires data as matrix, and response separated from predictors
trainm <- train %>% dplyr::select(-c(growth_2_6)) %>% as.matrix()
train_label <- as.matrix(train[,"growth_2_6"])
train_matrix <- xgb.DMatrix(data = as.matrix(trainm),label = train_label)

# do same for valid
validm <- valid %>% dplyr::select(-growth_2_6) %>% as.matrix()
valid_label <- as.matrix(valid[,"growth_2_6"])
valid_matrix <- xgb.DMatrix(data = as.matrix(validm),label = valid_label)
```

### Parameter tuning through cv
```{r}
watchlist <- list(train = train_matrix, test = valid_matrix)
set.seed(22)
best_param = list()
best_RMSE = Inf
best_RMSE_index = 0

for (iter in 1:100){
  xgb_params <- list("objective" = "reg:squarederror",
                   "eval_metric" = "rmse",
                   "eta" = runif(1,0.01,0.3), # learning rate: sample 1 between 0.01 and 0.3
                   "lambda" = runif(1,1,2), # (default 1) larger lambda -> less overfit
                   "gamma"=runif(1,0,0.2), # (default 0) similar to lambda 
                   "subsample"=runif(1,0.6,0.9), # fraction of observations to sample per tree
                   "colsample_bytree"=runif(1,0.5,0.8), # simliar to mtry in RF (but proportion)
                   "min_child_weight"=sample(1:40, 1) # higher values -> less overfit
                   )
  seed.number = sample.int(10000, 1)[[1]] # also set random seed
  set.seed(seed.number)
  cv.nround = 100 # need to cv on enough rounds too
  cv.nfold = 5
  xgb_cv <- xgb.cv(params = xgb_params, 
                 data = train_matrix, 
                 nrounds = cv.nround, 
                 nfold = cv.nfold, 
                 early_stopping_rounds = 8, # stop if rmse does not decrease in 8 rounds
                 maximize = F, # minimizing RMSE
                 verbose=F)
  min_RMSE = min(xgb_cv$evaluation_log[, test_rmse_mean]) # find minumum RMSE of n rounds
  min_RMSE_index = which.min(xgb_cv$evaluation_log[, test_rmse_mean]) # index of min RMSE
  
  if (min_RMSE < best_RMSE) {
      best_RMSE = min_RMSE
      best_RMSE_index = min_RMSE_index
      best_seednumber = seed.number
      best_param = xgb_params
  }
  if(iter %% 5 == 0){
    print(paste("Completed iteration ", iter))
    print(paste("Stopped at ", best_RMSE_index))
    print(paste("Current best RMSE ", best_RMSE))
  }
}
best_seednumber
best_RMSE_index # number of rounds with best RMSE
best_param # best parameter values for that round
```

### Create model with tuned parameters
```{r}
set.seed(best_seednumber)
xgb_fit <- xgb.train(data=train_matrix, 
                params=best_param, 
                nrounds=500,
                watchlist = watchlist)
```

### Variable importance
```{r}
xgb.plot.shap(data=trainm,model=xgb_fit,top_n=5)
```

### Validate
```{r}
valid_p <- predict(xgb_fit,valid_matrix)
xgb_RMSE <- RMSE(valid_p, valid$growth_2_6)
xgb_RMSE # validation RMSE
valid_preds <- data.frame("obs"=valid$growth_2_6, "pred"=valid_p)
head(valid_preds,10) # check predictions
```

### Predict on Test
```{r}
# format test for XGB
test <- test[subset]
testm <- as.matrix(test)
dtest <- xgb.DMatrix(data = testm)
# make prediction
preds <- predict(xgb_fit,newdata = dtest)
df <- data.frame("id" = test_id, "growth_2_6" = preds)
head(df)
# check distribution of predictions
ggplot(df) + geom_histogram(aes(x=growth_2_6))
```

### Write to csv
```{r}
# this is only here if the reader would like to examine the csv file themselves
# write.csv(df,"sub_4f.csv", row.names = FALSE)
```





# 6. Optional: References  

1. [XGBoost in R](https://xgboost.readthedocs.io/en/latest/parameter.html)
2. [XGBoost Parameters](https://stackoverflow.com/questions/35050846/xgboost-in-r-how-does-xgb-cv-pass-the-optimal-parameters-into-xgb-train)
3. [XGB Plot Function](https://www.rdocumentation.org/packages/xgboost/versions/1.2.0.1/topics/xgb.plot.shap)
4. [SHAP (SHapley Additive exPlanations)](https://christophm.github.io/interpretable-ml-book/shap.html)
5. [Recursive Feature Elimination for Feature Selection](https://machinelearningmastery.com/rfe-feature-selection-in-python/)
6. [Recursive Feature Elimination](https://www.scikit-yb.org/en/latest/api/model_selection/rfecv.html)




# 7. Contributions
**Tyler:** Data exploration (graphs), feature engineering, variable selection, a few Random Forest models, and tuning and validation of the final XGBoost model.


**Chelsea:** KNN tries, Boosting tries, RFE explorations, Random Forest importance explorations


**Charles:** Variable selection (Ridge Regression, LASSO, Elastic-Net, and Random Forest), tuning (Random Forest), and cross-validation (Ridge Regression, LASSO, and Elastic-Net)