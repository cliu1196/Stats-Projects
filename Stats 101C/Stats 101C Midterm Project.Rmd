---
title: "Stats 101C Midterm Project"
author: "Charles Liu (304804942)"
date: "10/31/2020"
output: pdf_document
---

# Loading Necessary Packages
```{r, warning = FALSE, message = FALSE}
library(readr)
library(ggplot2)
library(dplyr)
library(reshape2)
library(caret)
library(boot)
library(readxl)
library(MLeval)
library(MASS)
library(ISLR)
library(class)
library(mclust)
library(e1071)
library(MLeval)
library(nnet) #for more than 2 response values
```



# set working directory 
```{r}
getwd()
setwd("C:/Users/cliuk/Documents/UCLA Works/UCLA Fall 2020/Stats 101C/Midterm Project") #change it to your own!
```



# Read the data (MUST LOAD INDIVIDUALLY & SLOWLY TO AVOID CRASH!!!!)
```{r}
training <- read.csv("training.csv", header = TRUE)
test <- read.csv("test.csv", header = TRUE)
sample <- read.csv("sample.csv", header = TRUE)
```



# Read in descriptions & look at data (but I think it's easier to read if you actually open up the Excel file) 
```{r}
data_info <- read_excel("Feature_description.xlsx")
View(data_info)
View(training)
```



# Clean Data and Exploratory Analysis (Checking for NAs)
```{r}
na_vec <- c()
for (i in 1:ncol(training))
{
  na_vec[i] <- sum(is.na(training[,i]))
}
na_vec
#There are no NA's!!
```



# Proportion of each class
```{r}
table(training$class)

table(training$class)[[1]] / nrow(training)
table(training$class)[[2]] / nrow(training)
table(training$class)[[3]] / nrow(training)

```
So our data is very skewed: almost 90% of our training data is in class 0, which are neutral genes, and the Oncogenes (1) and Tumer Suppressor genes (2) only take up about 5% each. (I forgot what we need to do for this).  
Because of this, we need to use some resampling techniques.
Ideas:  
- duplicate observations from minority class
- use bootstrapping WITH replacement    
- undersample from the majority class


# Brief summary (broke them into parts)
```{r}
#what i look for is seeing how far the mean and median are in comparison to its spread
summary(training[,1:10])  #LOF_KB_Ratio and Missense_Entropy and LOF_TO_Silent_Ratio seem skewed to the right
summary(training[,11:20]) #LOF_TO_Benign_Ratio and Splice_TO_Benign_Ratio seem slightly skewed to the right
summary(training[,21:30]) # seems like NonSilent_TO_Silent_Ratio has outlier: 3rd quartile is 3.93, yet max is 395.50
# LOF_TO_Missense_Ratio max of 23.59 compared to 3rd quartile 0.12
#some 3rd quartiles lower than mean value
summary(training[,31:40]) #Gene_expression_Z_score also large max
summary(training[,41:50]) 
summary(training[,51:60])
summary(training[,61:70])
summary(training[,71:80])
summary(training[,81:90])
summary(training[,91:99]) 
#overall problems found: large maxes in comparison to the 3rd quartile. Will need to work  with outliers
```
All the predictors seem to be numerical... some values are strictly integer? like Gene_age and FamilyMemberCount




#Work with outliers?
```{r}

```



## Now let's make lots of graphs!

### First, let's make some histograms to understand the predictors  
This checks the distribution of each predictor and looks for anything fishy. We should understand what a normal range should be for each predictor and see if their distributions accurately represent that.

```{r understand_data}
for (var in names(training)[26:50]) { #so we don't make 100 graphs
  curr_plot = ggplot(training, aes_string(x=var))
  if(is.factor(training[,var]))  {
    curr_plot = curr_plot + geom_bar()
    
  } else  {
    curr_plot = curr_plot + geom_histogram(bins=20)
  }
  print(curr_plot)
}
names(training[26:50])
```
Notes so far:  



# Convert Features to Numeric
```{r convert_to_numeric}
#for each factor, convert it to numeric
for (var in names(training[26:50]))    #loop through each name of the variables with the alias "var"
{
  if(is.factor(training[,26:50]))
  {
    training[,var] = as.numeric(training[,26:50]) - 1 #TA likes to have 0 indexed, so values are 0 and 1
  }
}
```



# Let's also look at correlation (What is considered to have high multicollinearity?? abs(0.5)? or more?)
```{r}
# Place in R File to zoom in
#get the correlation matrix
cor_mtx = round(cor(training[,26:50]), 2) #so we don't get a 100 by 100 table

#reshape it
melted_cor_mtx <- melt(cor_mtx)

#draw the heatmap
cor_heatmap = ggplot(data = melted_cor_mtx, aes(x=Var1, y=Var2, fill=value)) + geom_tile()
cor_heatmap = cor_heatmap + 
        scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
        theme_minimal() +
        theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1))

cor_heatmap
```


# Here's some scatterplots for each variable vs class
```{r}
for (feat in names(training)[26:50])
#for (feat in vars_to_keep)
{
  if (feat != 'class')
  {
    p = ggplot(training, aes_string(feat, "class")) + geom_jitter(width=0.05, height=0.1, size=1)
    print(p)
  }
}
names(training[,26:50])
#if across class values there's not much difference in that particular x value, it reflects that predictor is not significant
```

  

# Here's some scatterplots for each a variable vs another colored by class  
This one might be more beneficial to just choose the two to compare, as it's hard to come up with every single combination with so many variables
```{r, message=FALSE, warning=FALSE}
attach(training)
training$classfactor <- factor(training$class)
# this doesn't include all pairwise though
#trying to figure out how to make class discrete
#training$classfactor <- factor(training$class)
for (i in 26:50)
#for (feat in vars_to_keep)
{
  var1 = names(training)[i]
  var2 = names(training)[i+1]
  var3 = names(training)[i+2]
  
  p1 = ggplot(data = training, aes_string(x = var1, y = var2)) + geom_jitter(aes(color = classfactor), width=0.05, height=0.1, size=1)
  print(p1)
  p2 = ggplot(data = training, aes_string(x = var1, y = var3)) + geom_jitter(aes(color = classfactor), width=0.05, height=0.1, size=1)
  print(p2)
  
}
#if across class values there's not much difference in that particular x value, it reflects that predictor is not significant
```

# Looking at (variables 51 - 99)
```{r}
traintags <- c("NG","OG","TSG")[as.integer(training$class)+1]
training$classfactor <- factor(traintags, levels=c("NG","OG","TSG"))
# names(training[51:99])
par(mfrow = c(2,3))
plot(training$classfactor, training$Gene_body_hypomethylation_in_cancer)# M
plot(training$classfactor, training$Gene_body_hypomethylation_in_cancer)# M
plot(training$classfactor, training$Canyon_genebody_hypermethylation)# N
plot(training$classfactor, training$intolerant_pLI)# Y
plot(training$classfactor, training$intolerant_pRec)# Y
plot(training$classfactor, training$intolerant_pNull)# Y
```

```{r}
par(mfrow = c(2,3))
plot(training$classfactor, training$Synonymous_Zscore)# N
plot(training$classfactor, training$Missense_Zscore)# Y
plot(training$classfactor, training$pLOF_Zscore)# Y
plot(training$classfactor, training$dN_to_dS_ratio)# N
plot(training$classfactor, training$GDI)# M
plot(training$classfactor, training$RVIS_percentile)# Y
```

```{r}
par(mfrow = c(2,3))
plot(training$classfactor, training$ncRVIS)# N
plot(training$classfactor, training$ncGERP)# Y
plot(training$classfactor, training$Gene_age)# Y
plot(training$classfactor, training$FamilyMemberCount)# N
plot(training$classfactor, training$Length_H3K4me3)# Y
plot(training$classfactor, training$Broad_H3K4me3_percentage)# Y
```

```{r}
par(mfrow = c(2,3))
plot(training$classfactor, training$H3K4me3_height)# Y
plot(training$classfactor, training$H3K4me2_width)# Y
plot(training$classfactor, training$Broad_H3K4me2_percentage)# Y
plot(training$classfactor, training$H3K4me2_height)# Y
plot(training$classfactor, training$H3K4me1_width)# Y
plot(training$classfactor, training$Broad_H3K4me1_percentage)# Y
```

```{r}
par(mfrow = c(2,3))
plot(training$classfactor, training$H3K4me1_height)# M
plot(training$classfactor, training$H3K36me3_width)# Y
plot(training$classfactor, training$Broad_H3K36me3_percentage)# Y
plot(training$classfactor, training$H3K36me3_height)# M
plot(training$classfactor, training$H3K27ac_width)# Y
plot(training$classfactor, training$Broad_H3K27ac_percentage)# Y
```

```{r}
par(mfrow = c(2,3))
plot(training$classfactor, training$H3K27ac_height)# Y
plot(training$classfactor, training$H3K27me3_width)# N
plot(training$classfactor, training$Broad_H3K27me3_percentage)# M
plot(training$classfactor, training$H3K27me3_height)# M
plot(training$classfactor, training$H3K9me3_width)# N
plot(training$classfactor, training$Broad_H3K9me3_percentage)# N
```

```{r}
par(mfrow = c(2,3))
plot(training$classfactor, training$H3K9me3_height)# N
plot(training$classfactor, training$H3K9ac_width)# Y
plot(training$classfactor, training$Broad_H3K9ac_percentage)# Y
plot(training$classfactor, training$H3K9ac_height)# Y
plot(training$classfactor, training$H3K9me2_width)# N
plot(training$classfactor, training$Broad_H3K9me2_percentage)# Y
```

```{r}
par(mfrow = c(2,3))
plot(training$classfactor, training$H3K9me2_height)# M
plot(training$classfactor, training$H3K79me2_width)# Y
plot(training$classfactor, training$Broad_H3K79me2_percentage)# Y
plot(training$classfactor, training$H3K79me2_height)# Y
plot(training$classfactor, training$H4K20me1_width)# Y
plot(training$classfactor, training$Broad_H4K20me1_percentage)# Y
```

```{r}
par(mfrow = c(1,2))
plot(training$classfactor, training$H4K20me1_height)# Y
plot(training$classfactor, training$class)# N
```

# Splitting data for cross validation (except i don't know how it works with 3 classes)
```{r}
# Set up w/ Training and Test
traintags <- c("NG","OG","TSG")[as.integer(class)+1]
training$classfactor <- factor(traintags, levels=c("NG","OG","TSG"))
limit <- 120
indexNG <- sample(which(class==0))[1:limit]
indexOG <- sample(which(class==1))[1:limit]
indexTSG <- sample(which(class==2))[1:limit]
trainindex <- c(indexNG,indexOG, indexTSG)
train_training <- training[trainindex,]
# We assume we have not seen this.
train_test <- training[-trainindex,]
train_control <- trainControl(method="cv", number = 5, 
                              classProbs = TRUE, 
                              savePredictions = TRUE)
```



# Creating the Models (LR & KNN)
## Logistic Regression
```{r}
LRfit <- train(classfactor ~ 
                      log_gene_length+
                        CNA_deletion+
                        CNA_amplification+Exon_Cons+
                        S50_score_replication_timing+
                        One_Minus_S50_score_replication_timing+
                        VEST_score+
                        Cell_proliferation_rate_CRISPR_KD+
                        Minus_Cell_proliferation_rate_CRISPR_KD+
                        Super_Enhancer_percentage+
                        BioGRID_betweenness+
                        BioGRID_log_degree+
                        Promoter_hypermethylation_in_cancer+
                        Promoter_hypomethylation_in_cancer+
                        Gene_body_hypermethylation_in_cancer,
               data = train_training, 
               method = "multinom", 
               family = "multinomial", 
               preProc = c("center", "scale"), 
               trControl = train_control) 
LRfit$results
```

### LR Prediction
```{r}
glm.train.train <- predict(LRfit, train_training)
trainError <- table(glm.train.train, train_training$class)
trainError
points <- trainError[1,1] + 20* (trainError[2,2] + trainError[3,3])
totalPoints <- table(train_training$class)[[1]] + 20 * (table(train_training$class)[[2]] + table(train_training$class)[[3]])
points/totalPoints
```


## KNN
```{r}
KNNfit <- train(classfactor ~ 
                  S50_score_replication_timing+
                  One_Minus_S50_score_replication_timing,
               data = train_training, 
               method = "knn", 
               preProc = c("center", "scale"), 
               trControl = train_control)
ggplot(KNNfit) + theme_bw()
KNNfit$results
```


### KNN Prediction
```{r}
knn.train.train <- predict(KNNfit, train_training)
trainError <- table(knn.train.train, train_training$class)
trainError
points <- trainError[1,1] + 20* (trainError[2,2] + trainError[3,3])
totalPoints <- table(train_training$class)[[1]] + 20 * (table(train_training$class)[[2]] + table(train_training$class)[[3]])
points/totalPoints
```


## Check for other items
```{r}
# Create a function for misclassifications accuracy
misclassfication_model <- function(prediction, observation, n) {
  misclass_risk <- sum((as.integer(prediction) -
                              as.integer(observation))^2)/nrow(n)
  return(misclass_risk)
}

# KNN prediction
predKNN <- predict(KNNfit, newdata = test)
CM_KNN <- confusionMatrix(data = predKNN, reference = test$class)
misclass_KNN <- misclassfication_model(prediction = KNNfit$pred$pred, 
                       observation = KNNfit$pred$obs, 
                       n = test)

# Logistic Regression prediction
predLR <- predict(LRfit, newdata = test)
CM_LR <- confusionMatrix(data = predLR, reference = test$class)
misclass_LR <- misclassfication_model(prediction = LRfit$pred$pred, 
                       observation = LRfit$pred$obs, 
                       n = test)

```


```{r}
#average training error rate
glm.train.train <- predict(glm.train, training_train, type="class")
trainError <- table(glm.train.train, training_train$class)
trainError
points <- trainError[1,1] + 20* (trainError[2,2] + trainError[3,3])
totalPoints <- table(training_train$class)[[1]] + 20 * (table(training_train$class)[[2]] + table(training_train$class)[[3]])
points/totalPoints


#predict test with logistic regression
glm.train.test = predict(glm.train, training_test, type="class")

testError <- table(glm.train.test, training_test$class)
testError

#average test error rate
sum(diag(testError)) / nrow(training_test)

#counting points (20 points for correctly evaluating 1 and 2, 1 point for class 0)
points <- testError[1,1] + 20* (testError[2,2] + testError[3,3])
totalPoints <- table(training_test$class)[[1]] + 20 * (table(training_test$class)[[2]] + table(training_test$class)[[3]])
points/totalPoints

#actually predicting for the test set
glm.test = predict(glm.train, test, type="class")
predictions = cbind(test$id, glm.test) %>% as.data.frame()
names(predictions) <- c("id", "class") #for some reason the values added by one
predictions$class = predictions$class - 1

#creating csv file to turn in
# write.csv(predictions, "predictions.csv", row.names = FALSE)
```



#Playing around with different subset of predictors
```{r}

```



